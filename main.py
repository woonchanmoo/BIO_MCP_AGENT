from langchain_mcp_adapters.client import MultiServerMCPClient
from src.agent import build_simple_agent
from langchain_core.messages import HumanMessage, AIMessageChunk, AIMessage
from langgraph.checkpoint.memory import MemorySaver
import asyncio
import warnings
from src.prompt import BASE_SYSTEM_PROMPT
from src.config.config import MCP_CONFIG, MCP_FILESYSTEM_DIR, LLM_MODEL
from prompt_toolkit import prompt as pt_prompt

warnings.filterwarnings("ignore", category=UserWarning)

async def get_multiline_input(prompt: str) -> str:
    # \033[96m: Cyanìƒ‰, \033[1m: Bold, \033[0m: Reset
    guide = "\033[96m\033[1m(ì „ì†¡: Esc ëˆ„ë¥¸ í›„ Enter)\033[0m"
    print(f"{prompt} {guide}")
    # multiline=Trueì¼ ë•Œ, ì „ì†¡ì€ ë³´í†µ 'Esc' ëˆ„ë¥¸ í›„ 'Enter' ë˜ëŠ” 'Meta+Enter'
    # í˜¹ì€ ë§ˆìš°ìŠ¤ë¡œ í´ë¦­í•  ìˆ˜ ì—†ëŠ” í™˜ê²½ì´ë¯€ë¡œ ì•ˆë‚´ ë©”ì‹œì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.
    user_input = await asyncio.to_thread(
        pt_prompt, 
        "> ", 
        multiline=True,
        prompt_continuation="  " # ì¤„ë°”ê¿ˆ ì‹œ ì•ì— ë¶™ëŠ” ì ‘ë‘ì–´
    )
    return user_input.strip()

async def stream_graph_response(input, graph, config={}):
    last_index = -1
    first_text = True

    async for message_chunk, metadata in graph.astream(
        input=input, stream_mode="messages", config=config
    ):
        # ë„êµ¬ ì‹¤í–‰ ë…¸ë“œì—ì„œ ë‚˜ì˜¤ëŠ” ì¶œë ¥ì€ ì¤‘ë³µì´ë¯€ë¡œ ê±´ë„ˆëœë‹ˆë‹¤.
        if metadata.get("langgraph_node") == "tools":
            continue

        # 1. AIMessage(ì™„ì„±ë³¸) ë˜ëŠ” AIMessageChunk(ì¡°ê°)ì¸ì§€ í™•ì¸
        if isinstance(message_chunk, (AIMessage, AIMessageChunk)):
            
            # 2. ë„êµ¬ í˜¸ì¶œ(Tool Calls) ì²˜ë¦¬
            # Chunk íƒ€ì…ì´ê³  tool_call_chunksê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì‹¤í–‰
            if isinstance(message_chunk, AIMessageChunk) and message_chunk.tool_call_chunks:
                for chunk in message_chunk.tool_call_chunks:
                    idx = chunk.get("index")
                    if idx != last_index:
                        if chunk.get("name"):
                            yield f"\n\033[94mğŸ› ï¸  Executing Tool: {chunk['name']}\033[0m\n"
                            last_index = idx
                    if chunk.get("args"):
                        yield f"\033[90m{chunk['args']}\033[0m"
            
            # 3. ì¼ë°˜ í…ìŠ¤íŠ¸ ë‚´ìš©(Content) ì¶œë ¥
            # ì™„ì„±ëœ AIMessage(ì—ëŸ¬ ì¤‘ë‹¨ ë©”ì‹œì§€ í¬í•¨)ì™€ Chunkì˜ í…ìŠ¤íŠ¸ë¥¼ ëª¨ë‘ ì¡ìŠµë‹ˆë‹¤.
            elif message_chunk.content:
                if first_text:
                    yield "\n\033[1;32m[AI]:\033[0m " 
                    first_text = False
                
                # contentê°€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¸ ê²½ìš°(ë©€í‹°ëª¨ë‹¬ ë“±)ë¥¼ ëŒ€ë¹„í•´ ë¬¸ìì—´ ë³€í™˜
                content_text = message_chunk.content if isinstance(message_chunk.content, str) else str(message_chunk.content)
                yield content_text

            # 4. ë§ˆë¬´ë¦¬ ì²˜ë¦¬ (Chunkì˜ finish_reason í™•ì¸)
            if isinstance(message_chunk, AIMessageChunk):
                if message_chunk.response_metadata.get("finish_reason") == "tool_calls":
                    yield "\n"
                    last_index = -1

async def run_mcp_agent():

    # Memory Configuration
    memory = MemorySaver()
    config = {
        "configurable": {"thread_id": "thread_1"},
        "recursion_limit": 300} # 50ë²ˆ ì´ìƒì˜ ë„êµ¬ ì‚¬ìš© ê°€ëŠ¥

    # MCP Server Connection
    try:
        print("CONNECTING MCP SERVER...")
        from src.config.config import MCP_CONFIG as config_dict
        print(f"ğŸ“‹ MCP Config contains {len(config_dict)} servers:")
        for server_name in config_dict.keys():
            print(f"   - {server_name}")
        
        client = MultiServerMCPClient(MCP_CONFIG)
        print("â³ Loading tools from servers...")
        # ì´ ë‹¨ê³„ì—ì„œ ì„œë²„ê°€ ì•ˆ ëœ¨ë©´ ë¬´í•œ ëŒ€ê¸°í•˜ê±°ë‚˜ ì£½ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        tools = await asyncio.wait_for(client.get_tools(), timeout=120.0) 
    except asyncio.TimeoutError:
        print("âŒ MCP ì„œë²„ ì—°ê²° íƒ€ì„ì•„ì›ƒ!")
        return
    except Exception as e:
        import traceback
        print(f"âŒ ì—°ê²° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("ğŸ“‹ Error trace:")
        traceback.print_exc()
        return

    if not tools:
        print("âŒ MCP ë„êµ¬ë¥¼ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return
    
    print(f"âœ… Loaded {len(tools)} tools.")

    system_prompt = f"""
    Your name is Scout and you are an expert data scientist.
    You help customers manage their data science projects by leveraging the tools available to you.
    Your goal is to collaborate with the customer in incrementally building their analysis or data modeling project.

    <filesystem>
    You have access to a set of tools that allow you to interact with the user's local filesystem. 
    You are only able to access files within the working directory `mcp_workspace`.
    The absolute path to this directory is: {MCP_FILESYSTEM_DIR}
    If you try to access a file outside of this directory, you will receive an error.
    Prefer relative paths from this root (for example: `inputs/data`, `runs/Q1/attempt3`, `docs`).
    </filesystem>

    {BASE_SYSTEM_PROMPT}

    <tools>
    {tools}
    </tools>

    Assist the customer in all aspects of their data science workflow.
    """
    
    # Agent Initialization
    mcp_agent = build_simple_agent(
        model=LLM_MODEL,
        system_prompt=system_prompt,
        tools=tools,
        checkpointer=memory
    )

    print("\n--- MCP Agent Started ---")
    print("ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'quit'ì„ ì…ë ¥í•˜ì„¸ìš”.")

    # 2. ë°˜ë³µ ë£¨í”„ ì‹œì‘
    while True:
        user_input = await get_multiline_input("\n[User]: ")

        if user_input.lower() in ["exit", "quit"]:
            print("ğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        if not user_input:
            continue

        msg = {
            "messages": [HumanMessage(content=user_input)]
        }

        try:
            print("\nğŸ¤– ...", end="\n\n", flush=True)
            
            # í†µí•©ëœ ì œë„ˆë ˆì´í„° í˜¸ì¶œ
            async for text in stream_graph_response(msg, mcp_agent, config):
                print(text, end="", flush=True)
            
            print("\n")
        
        except Exception as e:
                    # ì´ì œ ì—¬ê¸°ëŠ” 'ê·¸ë˜í”„ ë‚´ë¶€' ì—ëŸ¬ê°€ ì•„ë‹ˆë¼ 'ì‹œìŠ¤í…œ ë ˆë²¨' ì—ëŸ¬ë§Œ ì¡í™ë‹ˆë‹¤.
                    print(f"\n\033[91mğŸ”´ ì¹˜ëª…ì  ì‹œìŠ¤í…œ ì˜¤ë¥˜ ë°œìƒ: {e}\033[0m")
                    # í•„ìš”í•˜ë‹¤ë©´ ì—¬ê¸°ì„œë§Œ ì•„ì£¼ ì œí•œì ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì´ˆê¸°í™”ë¥¼ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

if __name__ == "__main__":
    # í„°ë¯¸ë„ ì‹¤í–‰ ì‹œì—ëŠ” ì•„ë˜ ë‘ ì¤„ì´ ì—†ì–´ë„ ë˜ì§€ë§Œ, ë…¸íŠ¸ë¶ í™˜ê²½ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€ ê°€ëŠ¥
    import nest_asyncio
    nest_asyncio.apply()

    try:
        # ë¹„ë™ê¸° ì—ì´ì „íŠ¸ ì‹¤í–‰ ë£¨í”„
        asyncio.run(run_mcp_agent())
    except KeyboardInterrupt:
        print("\nê°•ì œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
